{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os, sys, pandas, pathlib, time\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.transforms import v2\n",
    "from torchvision.io import read_image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tqdm import tqdm, trange\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
    "import random\n",
    "\n",
    "Image.MAX_IMAGE_PIXELS = None #Throws error if size of images exceeds this number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creates dictionary mapping labels(string) to vector(one-hot encoding)\n",
    "all_labels = list(np.load('materials/UBC-OCEAN_CS640/all_labels.npy'))\n",
    "num_classes = len(all_labels)\n",
    "label_dict = defaultdict(lambda: torch.zeros(num_classes))\n",
    "for i, label in enumerate(all_labels):\n",
    "    label_dict[label][i] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # change the size only if necessary\n",
    "transform = v2.Compose([v2.Resize(512, antialias = True),\n",
    "                            v2.CenterCrop(448),\n",
    "                            v2.ToImage(),\n",
    "                            v2.ToDtype(torch.float32, scale = True),\n",
    "                            v2.Normalize(mean = [0.4887, 0.4266, 0.4855], std = [0.4212, 0.3790, 0.4169])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 430/430 [20:51<00:00,  2.91s/it]\n"
     ]
    }
   ],
   "source": [
    "## TODO: create list of transformed images and update dataset class removing transforms\n",
    "\n",
    "def load_transform_images(folder_path):\n",
    "    images = {}  # Dictionary to store transformed images\n",
    "\n",
    "    for filename in tqdm(os.listdir(folder_path)):\n",
    "        if filename.endswith('.jpg'):  # Check for .jpg files\n",
    "            img_name = os.path.splitext(filename)[0]  # Extract the name (assuming it's a number)\n",
    "            #print(img_name)\n",
    "            img_path = os.path.join(folder_path, filename)\n",
    "            with Image.open(img_path) as img:\n",
    "                img = img.convert('RGB')  # Convert image to RGB (if not already)\n",
    "                transformed_img = transform(img)  # Apply the transformation\n",
    "                images[img_name] = transformed_img  # Store in dictionary with name as key\n",
    "\n",
    "    return images\n",
    "\n",
    "# Example usage\n",
    "folder_path = 'materials/UBC-OCEAN_CS640/train_images_compressed_80/'\n",
    "transformed_images = load_transform_images(folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training images: 344\n",
      "Number of validation images: 86\n",
      "Type of key: <class 'str'>\n",
      "Type of image: <class 'torchvision.tv_tensors._image.Image'>\n",
      "Size of image: torch.Size([3, 448, 448])\n"
     ]
    }
   ],
   "source": [
    "## Create list of training images and validation images using a split\n",
    "def split_dataset(images, train_ratio=0.8):\n",
    "    total_images = len(images)\n",
    "    train_size = int(total_images * train_ratio)\n",
    "\n",
    "    # Randomly shuffle the images\n",
    "    shuffled_items = list(images.items())\n",
    "    random.shuffle(shuffled_items)\n",
    "\n",
    "    # Split the images into training and validation sets\n",
    "    train_images = dict(shuffled_items[:train_size])\n",
    "    validation_images = dict(shuffled_items[train_size:])\n",
    "\n",
    "    return train_images, validation_images\n",
    "\n",
    "train_images, validation_images = split_dataset(transformed_images)\n",
    "\n",
    "\n",
    "print(f'Number of training images: {len(train_images.keys())}')\n",
    "print(f'Number of validation images: {len(validation_images.keys())}')\n",
    "print(f'Type of key: {type(list(train_images.keys())[0])}')\n",
    "print(f'Type of image: {type(list(train_images.values())[0])}')\n",
    "print(f'Size of image: {list(train_images.values())[0].size()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "##TODO: Update dataset to pass in list of images\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, csv_file, images, num_classes):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the csv file with annotations.\n",
    "            root_dir (string): Directory with all the images.\n",
    "            num_classes (int): Total number of classes.\n",
    "            transform (callable, optional): Optional transform to be applied on a sample.\n",
    "        \"\"\"\n",
    "        self.labels_frame = pd.read_csv(csv_file)\n",
    "\n",
    "        self.name_to_id = self.labels_frame['image_id'].to_dict()\n",
    "        self.id_to_name = {int(v):k for k, v in self.name_to_id.items()}\n",
    "        \n",
    "        \n",
    "        self.images = images\n",
    "        self.image_names = list(self.images.keys())  # List of image names\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(list(self.images.keys()))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.images[self.image_names[idx]]\n",
    "\n",
    "        label = self.labels_frame.iloc[self.id_to_name[int(self.image_names[idx])], 1]\n",
    "        #print(self.labels_frame.iloc[self.id_to_name[int(self.image_names[idx])]])\n",
    "        # Convert label to one-hot encoding\n",
    "        one_hot = label_dict[str(label)]\n",
    "        return image, one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: Add validation set and update method calls\n",
    "batch_size = 16\n",
    "\n",
    "# Create the dataset\n",
    "train_dataset = CustomDataset(csv_file='materials/UBC-OCEAN_CS640/train.csv', images=train_images, num_classes=num_classes)\n",
    "validation_dataset = CustomDataset(csv_file='materials/UBC-OCEAN_CS640/train.csv', images=validation_images, num_classes=num_classes)\n",
    "\n",
    "# Create a DataLoader\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "validation_dataloader = DataLoader(validation_dataset, batch_size=1, shuffle=True)\n",
    "\n",
    "# Now you can use this dataloader in your training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numer of training examples: 344\n",
      "Numer of testing examples: 86\n",
      "Type of pairing: <class 'tuple'>\n",
      "Number of inputs + outputs: 2\n",
      "Size of input: torch.Size([3, 448, 448])\n",
      "First input(normalized): Image([[[-1.1603, -1.1603, -1.1603,  ..., -1.1603, -1.1603, -1.1603],\n",
      "        [-1.1603, -1.1603, -1.1603,  ..., -1.1603, -1.1603, -1.1603],\n",
      "        [-1.1603, -1.1603, -1.1603,  ..., -1.1603, -1.1603, -1.1603],\n",
      "        ...,\n",
      "        [-1.1603, -1.1603, -1.1603,  ..., -1.1603, -1.1603, -1.1603],\n",
      "        [-1.1603, -1.1603, -1.1603,  ..., -1.1603, -1.1603, -1.1603],\n",
      "        [-1.1603, -1.1603, -1.1603,  ..., -1.1603, -1.1603, -1.1603]],\n",
      "\n",
      "       [[-1.1256, -1.1256, -1.1256,  ..., -1.1256, -1.1256, -1.1256],\n",
      "        [-1.1256, -1.1256, -1.1256,  ..., -1.1256, -1.1256, -1.1256],\n",
      "        [-1.1256, -1.1256, -1.1256,  ..., -1.1256, -1.1256, -1.1256],\n",
      "        ...,\n",
      "        [-1.1256, -1.1256, -1.1256,  ..., -1.1256, -1.1256, -1.1256],\n",
      "        [-1.1256, -1.1256, -1.1256,  ..., -1.1256, -1.1256, -1.1256],\n",
      "        [-1.1256, -1.1256, -1.1256,  ..., -1.1256, -1.1256, -1.1256]],\n",
      "\n",
      "       [[-1.1645, -1.1645, -1.1645,  ..., -1.1645, -1.1645, -1.1645],\n",
      "        [-1.1645, -1.1645, -1.1645,  ..., -1.1645, -1.1645, -1.1645],\n",
      "        [-1.1645, -1.1645, -1.1645,  ..., -1.1645, -1.1645, -1.1645],\n",
      "        ...,\n",
      "        [-1.1645, -1.1645, -1.1645,  ..., -1.1645, -1.1645, -1.1645],\n",
      "        [-1.1645, -1.1645, -1.1645,  ..., -1.1645, -1.1645, -1.1645],\n",
      "        [-1.1645, -1.1645, -1.1645,  ..., -1.1645, -1.1645, -1.1645]]], )\n",
      "First output(one-hot encoding): tensor([1., 0., 0., 0., 0.])\n"
     ]
    }
   ],
   "source": [
    "#Size of the reshaped first image in the data set\n",
    "#(RGB * width * height)\n",
    "print(f'Numer of training examples: {train_dataset.__len__()}')\n",
    "print(f'Numer of testing examples: {validation_dataset.__len__()}')\n",
    "print(f'Type of pairing: {type(train_dataset.__getitem__(0))}')\n",
    "print(f'Number of inputs + outputs: {len(validation_dataset.__getitem__(0))}')\n",
    "print(f'Size of input: {train_dataset.__getitem__(0)[0].size()}')\n",
    "print(f'First input(normalized): {train_dataset.__getitem__(0)[0]}')\n",
    "print(f'First output(one-hot encoding): {train_dataset.__getitem__(0)[1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is available\n"
     ]
    }
   ],
   "source": [
    "# torch.cuda.is_available() checks and returns a Boolean True if a GPU is available, else it'll return False\n",
    "is_cuda = torch.cuda.is_available()\n",
    "\n",
    "# If we have a GPU available, we'll set our device to GPU. We'll use this device variable later in our code.\n",
    "if is_cuda:\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"GPU is available\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"GPU not available, CPU used\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CancerCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CancerCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv3 = nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv4 = nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv5 = nn.Conv2d(512, 1024, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "        self.dropout = nn.Dropout(p=0.3)\n",
    "        \n",
    "        self.fc1 = nn.Linear(1024 * 14 * 14, 2048) \n",
    "        self.fc2 = nn.Linear(2048, 512)\n",
    "        self.fc3 = nn.Linear(512, 5)  \n",
    "\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Forward pass through the conv layers\n",
    "        x = self.pool(self.relu(self.conv1(x)))\n",
    "        x = self.pool(self.relu(self.conv2(x)))\n",
    "        x = self.pool(self.relu(self.conv3(x)))\n",
    "        x = self.pool(self.relu(self.conv4(x)))\n",
    "        x = self.pool(self.relu(self.conv5(x)))\n",
    "\n",
    "        x = x.view(-1, 1024 * 14 * 14) \n",
    "        \n",
    "        # Forward pass through the fully connected layers\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.fc3(x)  # No activation function here, it will be applied outside if needed (e.g., softmax)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CancerCNN(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv2): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv3): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv4): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv5): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (dropout): Dropout(p=0.3, inplace=False)\n",
      "  (fc1): Linear(in_features=200704, out_features=2048, bias=True)\n",
      "  (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "  (fc3): Linear(in_features=512, out_features=5, bias=True)\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (relu): ReLU()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Initialize the CNN Model\n",
    "model = CancerCNN().float()\n",
    "print(model)\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "# Step 5: Define Loss Function and Optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "num_epochs = 100\n",
    "\n",
    "losses = []\n",
    "\n",
    "\n",
    "try:\n",
    "    model = torch.load('Cancer_CNN_Final.pt')\n",
    "    img = mpimg.imread('loss_Final.png')\n",
    "    imgplot = plt.imshow(img)\n",
    "    plt.show()\n",
    "    \n",
    "except:\n",
    "    model.train()\n",
    "    for epoch in tqdm(range(num_epochs)):  # Number of epochs\n",
    "        for data, target in tqdm(train_dataloader):\n",
    "            data = data.to(device)\n",
    "            target = target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            #print(f'output: {output}')\n",
    "            #print(f'target: {target.type_as(output)}\\n')\n",
    "            loss = criterion(output, target.type_as(output)) # Ensuring target is same type as output\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        losses.append(loss.item())\n",
    "        #print(f'Epoch {epoch+1}, Loss: {loss.item()}')\n",
    "    torch.save(model, 'Cancer_CNN_Final.pt')\n",
    "\n",
    "    fig = plt.figure()\n",
    "    ax = plt.subplot(111)\n",
    "    ax.plot(losses)\n",
    "    plt.title('Loss')\n",
    "    plt.show()\n",
    "    fig.savefig('loss_Final.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "YPredict = []\n",
    "YTrue = []\n",
    "\n",
    "for data, target in tqdm(validation_dataloader):\n",
    "    data = data.to(device)  \n",
    "    output = model(data)\n",
    "    pred = int(torch.argmax(output))\n",
    "    label = int(torch.argmax(target))\n",
    "    YPredict.append(pred)\n",
    "    YTrue.append(label)\n",
    "    \n",
    "print(\"Confusion matrix: \" + str(confusion_matrix(YTrue, YPredict)))\n",
    "print(\"Accuracy: \" + str(accuracy_score(YTrue, YPredict)))\n",
    "print(\"F1: \" + str(f1_score(YTrue, YPredict, average = \"macro\")))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
