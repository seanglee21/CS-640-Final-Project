{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os, sys, pandas, pathlib, time\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.transforms import v2\n",
    "from torchvision.io import read_image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tqdm import tqdm, trange\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
    "import random\n",
    "\n",
    "Image.MAX_IMAGE_PIXELS = None #Throws error if size of images exceeds this number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creates dictionary mapping labels(string) to vector(one-hot encoding)\n",
    "all_labels = list(np.load('materials/UBC-OCEAN_CS640/all_labels.npy'))\n",
    "num_classes = len(all_labels)\n",
    "label_dict = defaultdict(lambda: torch.zeros(num_classes))\n",
    "for i, label in enumerate(all_labels):\n",
    "    label_dict[label][i] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # change the size only if necessary\n",
    "transform = v2.Compose([v2.Resize(1024, antialias = True),\n",
    "                            v2.CenterCrop(896),\n",
    "                            v2.ToImage(),\n",
    "                            v2.ToDtype(torch.float32, scale = True),\n",
    "                            v2.Normalize(mean = [0.4887, 0.4266, 0.4855], std = [0.4212, 0.3790, 0.4169])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 293/430 [13:37<05:57,  2.61s/it]"
     ]
    }
   ],
   "source": [
    "## TODO: create list of transformed images and update dataset class removing transforms\n",
    "\n",
    "def load_transform_images(folder_path):\n",
    "    images = {}  # Dictionary to store transformed images\n",
    "\n",
    "    for filename in tqdm(os.listdir(folder_path)):\n",
    "        if filename.endswith('.jpg'):  # Check for .jpg files\n",
    "            img_name = os.path.splitext(filename)[0]  # Extract the name (assuming it's a number)\n",
    "            #print(img_name)\n",
    "            img_path = os.path.join(folder_path, filename)\n",
    "            with Image.open(img_path) as img:\n",
    "                img = img.convert('RGB')  # Convert image to RGB (if not already)\n",
    "                transformed_img = transform(img)  # Apply the transformation\n",
    "                images[img_name] = transformed_img  # Store in dictionary with name as key\n",
    "\n",
    "    return images\n",
    "\n",
    "# Example usage\n",
    "folder_path = 'materials/UBC-OCEAN_CS640/train_images_compressed_80/'\n",
    "transformed_images = load_transform_images(folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training images: 344\n",
      "Number of validation images: 86\n",
      "Type of key: <class 'str'>\n",
      "Type of image: <class 'torchvision.tv_tensors._image.Image'>\n",
      "Size of image: torch.Size([3, 896, 896])\n"
     ]
    }
   ],
   "source": [
    "## Create list of training images and validation images using a split\n",
    "def split_dataset(images, train_ratio=0.8):\n",
    "    total_images = len(images)\n",
    "    train_size = int(total_images * train_ratio)\n",
    "\n",
    "    # Randomly shuffle the images\n",
    "    shuffled_items = list(images.items())\n",
    "    random.shuffle(shuffled_items)\n",
    "\n",
    "    # Split the images into training and validation sets\n",
    "    train_images = dict(shuffled_items[:train_size])\n",
    "    validation_images = dict(shuffled_items[train_size:])\n",
    "\n",
    "    return train_images, validation_images\n",
    "\n",
    "train_images, validation_images = split_dataset(transformed_images)\n",
    "\n",
    "\n",
    "print(f'Number of training images: {len(train_images.keys())}')\n",
    "print(f'Number of validation images: {len(validation_images.keys())}')\n",
    "print(f'Type of key: {type(list(train_images.keys())[0])}')\n",
    "print(f'Type of image: {type(list(train_images.values())[0])}')\n",
    "print(f'Size of image: {list(train_images.values())[0].size()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##TODO: Update dataset to pass in list of images\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, csv_file, images, num_classes):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the csv file with annotations.\n",
    "            root_dir (string): Directory with all the images.\n",
    "            num_classes (int): Total number of classes.\n",
    "            transform (callable, optional): Optional transform to be applied on a sample.\n",
    "        \"\"\"\n",
    "        self.labels_frame = pd.read_csv(csv_file)\n",
    "\n",
    "        self.name_to_id = self.labels_frame['image_id'].to_dict()\n",
    "        self.id_to_name = {int(v):k for k, v in self.name_to_id.items()}\n",
    "        \n",
    "        \n",
    "        self.images = images\n",
    "        self.image_names = list(self.images.keys())  # List of image names\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(list(self.images.keys()))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.images[self.image_names[idx]]\n",
    "\n",
    "        label = self.labels_frame.iloc[self.id_to_name[int(self.image_names[idx])], 1]\n",
    "        #print(self.labels_frame.iloc[self.id_to_name[int(self.image_names[idx])]])\n",
    "        # Convert label to one-hot encoding\n",
    "        one_hot = label_dict[str(label)]\n",
    "        return image, one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: Add validation set and update method calls\n",
    "batch_size = 16\n",
    "\n",
    "# Create the dataset\n",
    "train_dataset = CustomDataset(csv_file='materials/UBC-OCEAN_CS640/train.csv', images=train_images, num_classes=num_classes)\n",
    "validation_dataset = CustomDataset(csv_file='materials/UBC-OCEAN_CS640/train.csv', images=validation_images, num_classes=num_classes)\n",
    "\n",
    "# Create a DataLoader\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "validation_dataloader = DataLoader(validation_dataset, batch_size=1, shuffle=True)\n",
    "\n",
    "# Now you can use this dataloader in your training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numer of training examples: 344\n",
      "Numer of testing examples: 86\n",
      "Type of pairing: <class 'tuple'>\n",
      "Number of inputs + outputs: 2\n",
      "Size of input: torch.Size([3, 896, 896])\n",
      "First input(normalized): Image([[[ 0.7298,  0.7391,  0.6925,  ..., -1.1603, -1.1603, -1.1603],\n",
      "        [ 0.6274,  0.6646,  0.7298,  ..., -1.1603, -1.1603, -1.1603],\n",
      "        [ 0.7018,  0.7484,  0.7298,  ..., -1.1603, -1.1603, -1.1603],\n",
      "        ...,\n",
      "        [-1.1603, -1.1603, -1.1603,  ...,  0.6646,  0.7205,  0.7298],\n",
      "        [-1.1603, -1.1603, -1.1603,  ...,  0.7670,  0.7484,  0.7391],\n",
      "        [-1.1603, -1.1603, -1.1603,  ...,  0.8136,  0.7577,  0.7670]],\n",
      "\n",
      "       [[ 0.4161,  0.3851,  0.3334,  ..., -1.1256, -1.1256, -1.1256],\n",
      "        [ 0.2092,  0.2506,  0.3540,  ..., -1.1256, -1.1256, -1.1256],\n",
      "        [ 0.3334,  0.4161,  0.3851,  ..., -1.1256, -1.1256, -1.1256],\n",
      "        ...,\n",
      "        [-1.1256, -1.1256, -1.1256,  ...,  0.2920,  0.4161,  0.4161],\n",
      "        [-1.1256, -1.1256, -1.1256,  ...,  0.5196,  0.4575,  0.4782],\n",
      "        [-1.1256, -1.1256, -1.1256,  ...,  0.6438,  0.5093,  0.5713]],\n",
      "\n",
      "       [[ 0.8390,  0.8484,  0.7450,  ..., -1.1645, -1.1645, -1.1645],\n",
      "        [ 0.6603,  0.7450,  0.8202,  ..., -1.1645, -1.1645, -1.1645],\n",
      "        [ 0.7732,  0.8673,  0.8578,  ..., -1.1645, -1.1645, -1.1645],\n",
      "        ...,\n",
      "        [-1.1645, -1.1645, -1.1645,  ...,  0.6509,  0.7262,  0.7262],\n",
      "        [-1.1645, -1.1645, -1.1645,  ...,  0.8390,  0.7732,  0.7638],\n",
      "        [-1.1645, -1.1645, -1.1645,  ...,  0.9425,  0.8108,  0.8202]]], )\n",
      "First output(one-hot encoding): tensor([0., 1., 0., 0., 0.])\n"
     ]
    }
   ],
   "source": [
    "#Size of the reshaped first image in the data set\n",
    "#(RGB * width * height)\n",
    "print(f'Numer of training examples: {train_dataset.__len__()}')\n",
    "print(f'Numer of testing examples: {validation_dataset.__len__()}')\n",
    "print(f'Type of pairing: {type(train_dataset.__getitem__(0))}')\n",
    "print(f'Number of inputs + outputs: {len(validation_dataset.__getitem__(0))}')\n",
    "print(f'Size of input: {train_dataset.__getitem__(0)[0].size()}')\n",
    "print(f'First input(normalized): {train_dataset.__getitem__(0)[0]}')\n",
    "print(f'First output(one-hot encoding): {train_dataset.__getitem__(0)[1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is available\n"
     ]
    }
   ],
   "source": [
    "# torch.cuda.is_available() checks and returns a Boolean True if a GPU is available, else it'll return False\n",
    "is_cuda = torch.cuda.is_available()\n",
    "\n",
    "# If we have a GPU available, we'll set our device to GPU. We'll use this device variable later in our code.\n",
    "if is_cuda:\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"GPU is available\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"GPU not available, CPU used\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CancerCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CancerCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv3 = nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv4 = nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv5 = nn.Conv2d(512, 1024, kernel_size=3, stride=1, padding=1) \n",
    "        self.dropout = nn.Dropout(p=0.3)\n",
    "        self.fc1 = nn.Linear(1024 * 28 * 28, 2048)  \n",
    "        self.fc2 = nn.Linear(2048, 512)    \n",
    "        self.fc3 = nn.Linear(512, 128)\n",
    "        self.fc4 = nn.Linear(128, 32)\n",
    "        self.fc5 = nn.Linear(32, 5)        \n",
    "        \n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(self.relu(self.conv1(x)))\n",
    "        x = self.pool(self.relu(self.conv2(x)))\n",
    "        x = self.pool(self.relu(self.conv3(x)))\n",
    "        x = self.pool(self.relu(self.conv4(x)))\n",
    "        x = self.pool(self.relu(self.conv5(x)))  \n",
    "        x = x.view(-1, 1024 * 28 * 28)          # Adjust flatten step\n",
    "        x = self.dropout(x)\n",
    "        x = self.dropout(self.relu(self.fc1(x)))\n",
    "        x = self.dropout(self.relu(self.fc2(x)))\n",
    "        x = self.dropout(self.relu(self.fc3(x)))\n",
    "        x = self.dropout(self.relu(self.fc4(x)))\n",
    "        x = self.fc5(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CancerCNN(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv2): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv3): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv4): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv5): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (dropout): Dropout(p=0.3, inplace=False)\n",
      "  (fc1): Linear(in_features=802816, out_features=2048, bias=True)\n",
      "  (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "  (fc3): Linear(in_features=512, out_features=128, bias=True)\n",
      "  (fc4): Linear(in_features=128, out_features=32, bias=True)\n",
      "  (fc5): Linear(in_features=32, out_features=5, bias=True)\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (relu): ReLU()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Initialize the CNN Model\n",
    "model = CancerCNN().float()\n",
    "print(model)\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "# Step 5: Define Loss Function and Optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:38<?, ?it/s]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 6.12 GiB. GPU 0 has a total capacty of 8.00 GiB of which 0 bytes is free. Of the allocated memory 30.95 GiB is allocated by PyTorch, and 3.92 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\TaiSh\\Source\\CS_640-Artificial_Intelligence\\Final_Project\\Final_Project_Final.ipynb Cell 12\u001b[0m line \u001b[0;36m7\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/TaiSh/Source/CS_640-Artificial_Intelligence/Final_Project/Final_Project_Final.ipynb#X14sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/TaiSh/Source/CS_640-Artificial_Intelligence/Final_Project/Final_Project_Final.ipynb#X14sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     model \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mload(\u001b[39m'\u001b[39;49m\u001b[39mCancer_CNN_Final.pt\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/TaiSh/Source/CS_640-Artificial_Intelligence/Final_Project/Final_Project_Final.ipynb#X14sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     img \u001b[39m=\u001b[39m mpimg\u001b[39m.\u001b[39mimread(\u001b[39m'\u001b[39m\u001b[39mloss_Final.png\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\TaiSh\\.conda\\envs\\conda311\\Lib\\site-packages\\torch\\serialization.py:986\u001b[0m, in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[0;32m    984\u001b[0m     pickle_load_args[\u001b[39m'\u001b[39m\u001b[39mencoding\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m--> 986\u001b[0m \u001b[39mwith\u001b[39;00m _open_file_like(f, \u001b[39m'\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39mas\u001b[39;00m opened_file:\n\u001b[0;32m    987\u001b[0m     \u001b[39mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[0;32m    988\u001b[0m         \u001b[39m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[0;32m    989\u001b[0m         \u001b[39m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[0;32m    990\u001b[0m         \u001b[39m# reset back to the original position.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\TaiSh\\.conda\\envs\\conda311\\Lib\\site-packages\\torch\\serialization.py:435\u001b[0m, in \u001b[0;36m_open_file_like\u001b[1;34m(name_or_buffer, mode)\u001b[0m\n\u001b[0;32m    434\u001b[0m \u001b[39mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[1;32m--> 435\u001b[0m     \u001b[39mreturn\u001b[39;00m _open_file(name_or_buffer, mode)\n\u001b[0;32m    436\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\TaiSh\\.conda\\envs\\conda311\\Lib\\site-packages\\torch\\serialization.py:416\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[1;34m(self, name, mode)\u001b[0m\n\u001b[0;32m    415\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, name, mode):\n\u001b[1;32m--> 416\u001b[0m     \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(\u001b[39mopen\u001b[39;49m(name, mode))\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Cancer_CNN_Final.pt'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\TaiSh\\Source\\CS_640-Artificial_Intelligence\\Final_Project\\Final_Project_Final.ipynb Cell 12\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/TaiSh/Source/CS_640-Artificial_Intelligence/Final_Project/Final_Project_Final.ipynb#X14sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m     loss \u001b[39m=\u001b[39m criterion(output, target\u001b[39m.\u001b[39mtype_as(output)) \u001b[39m# Ensuring target is same type as output\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/TaiSh/Source/CS_640-Artificial_Intelligence/Final_Project/Final_Project_Final.ipynb#X14sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m     loss\u001b[39m.\u001b[39mbackward()\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/TaiSh/Source/CS_640-Artificial_Intelligence/Final_Project/Final_Project_Final.ipynb#X14sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m     optimizer\u001b[39m.\u001b[39;49mstep()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/TaiSh/Source/CS_640-Artificial_Intelligence/Final_Project/Final_Project_Final.ipynb#X14sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m losses\u001b[39m.\u001b[39mappend(loss\u001b[39m.\u001b[39mitem())\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/TaiSh/Source/CS_640-Artificial_Intelligence/Final_Project/Final_Project_Final.ipynb#X14sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m \u001b[39m#print(f'Epoch {epoch+1}, Loss: {loss.item()}')\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\TaiSh\\.conda\\envs\\conda311\\Lib\\site-packages\\torch\\optim\\optimizer.py:373\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    368\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    369\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[0;32m    370\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mfunc\u001b[39m}\u001b[39;00m\u001b[39m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[39m{\u001b[39;00mresult\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    371\u001b[0m             )\n\u001b[1;32m--> 373\u001b[0m out \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    374\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_optimizer_step_code()\n\u001b[0;32m    376\u001b[0m \u001b[39m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\TaiSh\\.conda\\envs\\conda311\\Lib\\site-packages\\torch\\optim\\optimizer.py:76\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     74\u001b[0m     torch\u001b[39m.\u001b[39mset_grad_enabled(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdefaults[\u001b[39m'\u001b[39m\u001b[39mdifferentiable\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m     75\u001b[0m     torch\u001b[39m.\u001b[39m_dynamo\u001b[39m.\u001b[39mgraph_break()\n\u001b[1;32m---> 76\u001b[0m     ret \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m     77\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     78\u001b[0m     torch\u001b[39m.\u001b[39m_dynamo\u001b[39m.\u001b[39mgraph_break()\n",
      "File \u001b[1;32mc:\\Users\\TaiSh\\.conda\\envs\\conda311\\Lib\\site-packages\\torch\\optim\\adam.py:163\u001b[0m, in \u001b[0;36mAdam.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    152\u001b[0m     beta1, beta2 \u001b[39m=\u001b[39m group[\u001b[39m'\u001b[39m\u001b[39mbetas\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m    154\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_init_group(\n\u001b[0;32m    155\u001b[0m         group,\n\u001b[0;32m    156\u001b[0m         params_with_grad,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    160\u001b[0m         max_exp_avg_sqs,\n\u001b[0;32m    161\u001b[0m         state_steps)\n\u001b[1;32m--> 163\u001b[0m     adam(\n\u001b[0;32m    164\u001b[0m         params_with_grad,\n\u001b[0;32m    165\u001b[0m         grads,\n\u001b[0;32m    166\u001b[0m         exp_avgs,\n\u001b[0;32m    167\u001b[0m         exp_avg_sqs,\n\u001b[0;32m    168\u001b[0m         max_exp_avg_sqs,\n\u001b[0;32m    169\u001b[0m         state_steps,\n\u001b[0;32m    170\u001b[0m         amsgrad\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mamsgrad\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    171\u001b[0m         beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[0;32m    172\u001b[0m         beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[0;32m    173\u001b[0m         lr\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mlr\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    174\u001b[0m         weight_decay\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mweight_decay\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    175\u001b[0m         eps\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39meps\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    176\u001b[0m         maximize\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mmaximize\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    177\u001b[0m         foreach\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mforeach\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    178\u001b[0m         capturable\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mcapturable\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    179\u001b[0m         differentiable\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mdifferentiable\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    180\u001b[0m         fused\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mfused\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    181\u001b[0m         grad_scale\u001b[39m=\u001b[39;49m\u001b[39mgetattr\u001b[39;49m(\u001b[39mself\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mgrad_scale\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m    182\u001b[0m         found_inf\u001b[39m=\u001b[39;49m\u001b[39mgetattr\u001b[39;49m(\u001b[39mself\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mfound_inf\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m    183\u001b[0m     )\n\u001b[0;32m    185\u001b[0m \u001b[39mreturn\u001b[39;00m loss\n",
      "File \u001b[1;32mc:\\Users\\TaiSh\\.conda\\envs\\conda311\\Lib\\site-packages\\torch\\optim\\adam.py:311\u001b[0m, in \u001b[0;36madam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[0;32m    308\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    309\u001b[0m     func \u001b[39m=\u001b[39m _single_tensor_adam\n\u001b[1;32m--> 311\u001b[0m func(params,\n\u001b[0;32m    312\u001b[0m      grads,\n\u001b[0;32m    313\u001b[0m      exp_avgs,\n\u001b[0;32m    314\u001b[0m      exp_avg_sqs,\n\u001b[0;32m    315\u001b[0m      max_exp_avg_sqs,\n\u001b[0;32m    316\u001b[0m      state_steps,\n\u001b[0;32m    317\u001b[0m      amsgrad\u001b[39m=\u001b[39;49mamsgrad,\n\u001b[0;32m    318\u001b[0m      beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[0;32m    319\u001b[0m      beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[0;32m    320\u001b[0m      lr\u001b[39m=\u001b[39;49mlr,\n\u001b[0;32m    321\u001b[0m      weight_decay\u001b[39m=\u001b[39;49mweight_decay,\n\u001b[0;32m    322\u001b[0m      eps\u001b[39m=\u001b[39;49meps,\n\u001b[0;32m    323\u001b[0m      maximize\u001b[39m=\u001b[39;49mmaximize,\n\u001b[0;32m    324\u001b[0m      capturable\u001b[39m=\u001b[39;49mcapturable,\n\u001b[0;32m    325\u001b[0m      differentiable\u001b[39m=\u001b[39;49mdifferentiable,\n\u001b[0;32m    326\u001b[0m      grad_scale\u001b[39m=\u001b[39;49mgrad_scale,\n\u001b[0;32m    327\u001b[0m      found_inf\u001b[39m=\u001b[39;49mfound_inf)\n",
      "File \u001b[1;32mc:\\Users\\TaiSh\\.conda\\envs\\conda311\\Lib\\site-packages\\torch\\optim\\adam.py:503\u001b[0m, in \u001b[0;36m_multi_tensor_adam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[0;32m    501\u001b[0m         torch\u001b[39m.\u001b[39m_foreach_add_(device_grads, device_params, alpha\u001b[39m=\u001b[39mweight_decay)\n\u001b[0;32m    502\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 503\u001b[0m         device_grads \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49m_foreach_add(device_grads, device_params, alpha\u001b[39m=\u001b[39;49mweight_decay)\n\u001b[0;32m    505\u001b[0m \u001b[39m# Decay the first and second moment running average coefficient\u001b[39;00m\n\u001b[0;32m    506\u001b[0m torch\u001b[39m.\u001b[39m_foreach_lerp_(device_exp_avgs, device_grads, \u001b[39m1\u001b[39m \u001b[39m-\u001b[39m beta1)\n",
      "\u001b[1;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 6.12 GiB. GPU 0 has a total capacty of 8.00 GiB of which 0 bytes is free. Of the allocated memory 30.95 GiB is allocated by PyTorch, and 3.92 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "num_epochs = 100\n",
    "\n",
    "losses = []\n",
    "\n",
    "\n",
    "try:\n",
    "    model = torch.load('Cancer_CNN_Final.pt')\n",
    "    img = mpimg.imread('loss_Final.png')\n",
    "    imgplot = plt.imshow(img)\n",
    "    plt.show()\n",
    "    \n",
    "except:\n",
    "    model.train()\n",
    "    for epoch in tqdm(range(num_epochs)):  # Number of epochs\n",
    "        for data, target in train_dataloader:\n",
    "            data = data.to(device)\n",
    "            target = target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            #print(f'output: {output}')\n",
    "            #print(f'target: {target.type_as(output)}\\n')\n",
    "            loss = criterion(output, target.type_as(output)) # Ensuring target is same type as output\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        losses.append(loss.item())\n",
    "        #print(f'Epoch {epoch+1}, Loss: {loss.item()}')\n",
    "    torch.save(model, 'Cancer_CNN_Final.pt')\n",
    "\n",
    "    fig = plt.figure()\n",
    "    ax = plt.subplot(111)\n",
    "    ax.plot(losses)\n",
    "    plt.title('Loss')\n",
    "    plt.show()\n",
    "    fig.savefig('loss_Final.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "YPredict = []\n",
    "YTrue = []\n",
    "\n",
    "for data, target in tqdm(validation_dataloader):\n",
    "    data = data.to(device)  \n",
    "    output = model(data)\n",
    "    pred = int(torch.argmax(output))\n",
    "    label = int(torch.argmax(target))\n",
    "    YPredict.append(pred)\n",
    "    YTrue.append(label)\n",
    "    \n",
    "print(\"Confusion matrix: \" + str(confusion_matrix(YTrue, YPredict)))\n",
    "print(\"Accuracy: \" + str(accuracy_score(YTrue, YPredict)))\n",
    "print(\"F1: \" + str(f1_score(YTrue, YPredict, average = \"macro\")))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
