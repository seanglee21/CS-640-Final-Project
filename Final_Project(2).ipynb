{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os, sys, pandas, pathlib, time\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.transforms import v2\n",
    "from torchvision.io import read_image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tqdm import tqdm, trange\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
    "\n",
    "Image.MAX_IMAGE_PIXELS = None #Throws error if size of images exceeds this number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creates dictionary mapping labels(string) to vector(one-hot encoding)\n",
    "all_labels = list(np.load('materials/UBC-OCEAN_CS640/all_labels.npy'))\n",
    "num_classes = len(all_labels)\n",
    "label_dict = defaultdict(lambda: torch.zeros(num_classes))\n",
    "for i, label in enumerate(all_labels):\n",
    "    label_dict[label][i] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, csv_file, root_dir, num_classes, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the csv file with annotations.\n",
    "            root_dir (string): Directory with all the images.\n",
    "            num_classes (int): Total number of classes.\n",
    "            transform (callable, optional): Optional transform to be applied on a sample.\n",
    "        \"\"\"\n",
    "        self.labels_frame = pd.read_csv(csv_file)\n",
    "        self.root_dir = root_dir\n",
    "        self.num_classes = num_classes\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels_frame.index)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = os.path.join(self.root_dir, str(self.labels_frame.iloc[idx, 0])) + '.jpg'\n",
    "        image = Image.open(img_name)\n",
    "        #image = Image.open(img_name)\n",
    "        label = self.labels_frame.iloc[idx, 1]\n",
    "\n",
    "        # Convert label to one-hot encoding\n",
    "        one_hot = label_dict[label]\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # change the size only if necessary\n",
    "transform = v2.Compose([v2.Resize(256, antialias = True),\n",
    "                            v2.CenterCrop(224),\n",
    "                            v2.ToImage(),\n",
    "                            v2.ToDtype(torch.float32, scale = True),\n",
    "                            v2.Normalize(mean = [0.4887, 0.4266, 0.4855], std = [0.4212, 0.3790, 0.4169])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "\n",
    "# Create the dataset\n",
    "train_dataset = CustomDataset(csv_file='materials/UBC-OCEAN_CS640/train.csv', root_dir='materials/UBC-OCEAN_CS640/train_images_compressed_80/', num_classes=num_classes, transform=transform)\n",
    "test_dataset = CustomDataset(csv_file='materials/UBC-OCEAN_CS640/test.csv', root_dir='materials/UBC-OCEAN_CS640/test_images_compressed_80/', num_classes=num_classes, transform=transform)\n",
    "\n",
    "# Create a DataLoader\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Now you can use this dataloader in your training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numer of training examples: 430\n",
      "Numer of testing examples: 108\n",
      "Type of pairing: <class 'tuple'>\n",
      "Number of inputs + outputs: 2\n",
      "Size of input: torch.Size([3, 224, 224])\n",
      "First input(normalized): Image([[[-1.1603, -1.1603, -1.1603,  ...,  0.7205,  0.7298,  0.7391],\n",
      "        [-1.1603, -1.1603, -1.1603,  ...,  0.7298,  0.7298,  0.7391],\n",
      "        [-1.1603, -1.1603, -1.1603,  ...,  0.7391,  0.7298,  0.7298],\n",
      "        ...,\n",
      "        [-1.1603, -1.1603, -1.1603,  ...,  0.7018,  0.6925,  0.7577],\n",
      "        [-1.1603, -1.1603, -1.1603,  ...,  0.7391,  0.8508,  0.9346],\n",
      "        [-1.1603, -1.1603, -1.1603,  ...,  0.8694,  0.9532,  0.9625]],\n",
      "\n",
      "       [[-1.1256, -1.1256, -1.1256,  ...,  0.4575,  0.4886,  0.5196],\n",
      "        [-1.1256, -1.1256, -1.1256,  ...,  0.4782,  0.4989,  0.5093],\n",
      "        [-1.1256, -1.1256, -1.1256,  ...,  0.4886,  0.5093,  0.4989],\n",
      "        ...,\n",
      "        [-1.1256, -1.1256, -1.1256,  ...,  0.4782,  0.4886,  0.6645],\n",
      "        [-1.1256, -1.1256, -1.1256,  ...,  0.5920,  0.8818,  1.0783],\n",
      "        [-1.1256, -1.1256, -1.1256,  ...,  0.9128,  1.1197,  1.1404]],\n",
      "\n",
      "       [[-1.1645, -1.1645, -1.1645,  ...,  0.8767,  0.8673,  0.8673],\n",
      "        [-1.1645, -1.1645, -1.1645,  ...,  0.8673,  0.8673,  0.8578],\n",
      "        [-1.1645, -1.1645, -1.1645,  ...,  0.8673,  0.8767,  0.8390],\n",
      "        ...,\n",
      "        [-1.1645, -1.1645, -1.1645,  ...,  0.7732,  0.7356,  0.7732],\n",
      "        [-1.1645, -1.1645, -1.1645,  ...,  0.7732,  0.8390,  0.9237],\n",
      "        [-1.1645, -1.1645, -1.1645,  ...,  0.8578,  0.9425,  0.9519]]], )\n",
      "First output(one-hot encoding): tensor([0., 0., 0., 1., 0.])\n"
     ]
    }
   ],
   "source": [
    "#Size of the reshaped first image in the data set\n",
    "#(RGB * width * height)\n",
    "print(f'Numer of training examples: {train_dataset.__len__()}')\n",
    "print(f'Numer of testing examples: {test_dataset.__len__()}')\n",
    "print(f'Type of pairing: {type(train_dataset.__getitem__(0))}')\n",
    "print(f'Number of inputs + outputs: {len(train_dataset.__getitem__(0))}')\n",
    "print(f'Size of input: {train_dataset.__getitem__(0)[0].size()}')\n",
    "print(f'First input(normalized): {train_dataset.__getitem__(0)[0]}')\n",
    "print(f'First output(one-hot encoding): {train_dataset.__getitem__(0)[1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is available\n"
     ]
    }
   ],
   "source": [
    "# torch.cuda.is_available() checks and returns a Boolean True if a GPU is available, else it'll return False\n",
    "is_cuda = torch.cuda.is_available()\n",
    "\n",
    "# If we have a GPU available, we'll set our device to GPU. We'll use this device variable later in our code.\n",
    "if is_cuda:\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"GPU is available\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"GPU not available, CPU used\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CancerCNN_1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CancerCNN_1, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n",
    "        # Adjust the size of the fully connected layer\n",
    "        self.fc1 = nn.Linear(128 * 28 * 28, 512)\n",
    "        self.fc2 = nn.Linear(512, 5) # Adjust according to the number of classes\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(self.relu(self.conv1(x)))\n",
    "        x = self.pool(self.relu(self.conv2(x)))\n",
    "        x = self.pool(self.relu(self.conv3(x)))\n",
    "        x = x.view(-1, 128 * 28 * 28) # Adjust this flattening based on the output size\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CancerCNN_1(\n",
      "  (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (fc1): Linear(in_features=100352, out_features=512, bias=True)\n",
      "  (fc2): Linear(in_features=512, out_features=5, bias=True)\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (relu): ReLU()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Initialize the CNN Model\n",
    "model_1 = CancerCNN_1().float()\n",
    "print(model_1)\n",
    "\n",
    "model_1 = model_1.to(device)\n",
    "\n",
    "# Step 5: Define Loss Function and Optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model_1.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output: tensor([[-0.0112,  0.0270,  0.0812, -0.0124, -0.0536],\n",
      "        [-0.0200,  0.0084,  0.0711, -0.0239, -0.0625],\n",
      "        [-0.0216,  0.0171,  0.0723, -0.0152, -0.0481],\n",
      "        [-0.0128,  0.0155,  0.0561, -0.0197, -0.0493],\n",
      "        [-0.0215,  0.0174,  0.0724, -0.0153, -0.0486],\n",
      "        [-0.0070,  0.0106,  0.0569, -0.0281, -0.0617],\n",
      "        [ 0.0022,  0.0163,  0.0691, -0.0241, -0.0567],\n",
      "        [-0.0111,  0.0083,  0.0544, -0.0170, -0.0682]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "target: tensor([[1., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0.],\n",
      "        [0., 1., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0.],\n",
      "        [0., 0., 0., 0., 1.],\n",
      "        [1., 0., 0., 0., 0.]], device='cuda:0')\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 1/54 [00:19<17:28, 19.78s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output: tensor([[ 0.8792,  0.1461, -0.8902,  0.0547, -0.1971],\n",
      "        [ 1.0192,  0.3669, -1.1197,  0.1486, -0.4566],\n",
      "        [ 0.9711,  0.3886, -1.1271,  0.1240, -0.4296],\n",
      "        [ 0.9414,  0.4748, -1.1012,  0.1872, -0.5804],\n",
      "        [ 0.9651,  0.4685, -1.1164,  0.1849, -0.5736],\n",
      "        [ 0.9910,  0.4617, -1.1308,  0.1746, -0.5774],\n",
      "        [ 0.8197,  0.1631, -0.8632,  0.0524, -0.1771],\n",
      "        [ 0.9808,  0.4615, -1.1213,  0.1788, -0.5766]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "target: tensor([[1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1.],\n",
      "        [0., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1.],\n",
      "        [1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0.],\n",
      "        [0., 0., 0., 1., 0.],\n",
      "        [0., 1., 0., 0., 0.]], device='cuda:0')\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output: tensor([[ 0.4539,  0.6682, -1.7147,  0.4490,  0.2355],\n",
      "        [ 0.4455,  0.8523, -1.7251,  0.4560,  0.1979],\n",
      "        [ 0.6650,  0.3845, -1.6138,  0.4167,  0.2545],\n",
      "        [ 0.4661,  0.8724, -1.7468,  0.4329,  0.1952],\n",
      "        [ 0.5339,  0.7703, -1.7344,  0.4346,  0.2340],\n",
      "        [ 0.7627,  0.1897, -1.3472,  0.3284,  0.2271],\n",
      "        [ 0.4467,  0.8564, -1.7408,  0.4580,  0.2012],\n",
      "        [ 0.6743,  0.3481, -1.4614,  0.3467,  0.2102]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "target: tensor([[1., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0.],\n",
      "        [1., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0.],\n",
      "        [1., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0.]], device='cuda:0')\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output: tensor([[ 1.4544,  0.0099, -1.4657,  0.1786,  0.1788],\n",
      "        [ 1.1190,  0.3966, -1.4577,  0.1872,  0.2665],\n",
      "        [ 1.2380, -0.1139, -1.1026,  0.1865,  0.0894],\n",
      "        [ 1.5183, -0.1028, -1.3939,  0.1607,  0.2682],\n",
      "        [ 1.1612,  0.3964, -1.4975,  0.1799,  0.2673],\n",
      "        [ 1.2273,  0.3283, -1.4902,  0.1748,  0.2908],\n",
      "        [ 1.6251, -0.0977, -1.5469,  0.2060,  0.2194],\n",
      "        [ 1.1832,  0.3735, -1.5220,  0.0985,  0.2589]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "target: tensor([[0., 1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1.],\n",
      "        [0., 0., 1., 0., 0.],\n",
      "        [1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0.]], device='cuda:0')\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output: tensor([[ 2.0157, -0.3004, -1.1422,  0.0494, -0.1111],\n",
      "        [ 2.1092, -0.4741, -1.0628,  0.0202,  0.0231],\n",
      "        [ 1.5146, -0.0396, -1.0395, -0.0157,  0.2961],\n",
      "        [ 1.7254, -0.1185, -1.1485,  0.0403, -0.0332],\n",
      "        [ 2.0522, -0.3116, -1.1951,  0.1050, -0.1094],\n",
      "        [ 1.4528, -0.0189, -1.0053,  0.0027,  0.2901],\n",
      "        [ 1.5278, -0.0334, -1.0694,  0.0062,  0.2918],\n",
      "        [ 2.0516, -0.4080, -1.0966, -0.0118,  0.1347]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "target: tensor([[0., 0., 1., 0., 0.],\n",
      "        [1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0.],\n",
      "        [1., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0.],\n",
      "        [0., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0.],\n",
      "        [1., 0., 0., 0., 0.]], device='cuda:0')\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output: tensor([[ 2.0669, -0.5910, -0.5436, -0.0033, -0.2644],\n",
      "        [ 1.9531, -0.4515, -0.6416,  0.0162, -0.3170],\n",
      "        [ 1.4731, -0.3496, -0.5843,  0.0595,  0.1118],\n",
      "        [ 2.0318, -0.5580, -0.6062,  0.0621, -0.3144],\n",
      "        [ 2.1291, -0.6193, -0.6157, -0.0044, -0.1507],\n",
      "        [ 1.8612, -0.4566, -0.5771,  0.0765, -0.2913],\n",
      "        [ 1.5328, -0.3678, -0.6030,  0.0439,  0.1086],\n",
      "        [ 1.9207, -0.4439, -0.5739, -0.0224, -0.3026]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "target: tensor([[0., 0., 0., 1., 0.],\n",
      "        [0., 1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1.],\n",
      "        [1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0.],\n",
      "        [0., 0., 0., 1., 0.]], device='cuda:0')\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output: tensor([[ 1.5600, -0.4134, -0.3055,  0.1376, -0.5334],\n",
      "        [ 1.5677, -0.3585, -0.3753,  0.0826, -0.4961],\n",
      "        [ 1.3643, -0.3567, -0.2825,  0.1336, -0.4743],\n",
      "        [ 1.1696, -0.3566, -0.3724,  0.2133, -0.1444],\n",
      "        [ 1.3548, -0.4582, -0.2725,  0.1547, -0.3331],\n",
      "        [ 1.5056, -0.4410, -0.3497,  0.1795, -0.3910],\n",
      "        [ 0.9543, -0.3039, -0.3132,  0.2358,  0.0121],\n",
      "        [ 1.3519, -0.3482, -0.2736,  0.1253, -0.4561]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "target: tensor([[0., 0., 1., 0., 0.],\n",
      "        [1., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0.],\n",
      "        [1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1.],\n",
      "        [0., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0.]], device='cuda:0')\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output: tensor([[ 0.6359, -0.2813, -0.0850,  0.2711, -0.0415],\n",
      "        [ 0.6919, -0.3327, -0.0798,  0.3040, -0.1446],\n",
      "        [ 0.6358, -0.2813, -0.0850,  0.2711, -0.0414],\n",
      "        [ 0.6358, -0.2813, -0.0850,  0.2711, -0.0414],\n",
      "        [ 1.2090, -0.5114,  0.0220,  0.1754, -0.4896],\n",
      "        [ 1.0810, -0.5208, -0.0474,  0.1843, -0.2033],\n",
      "        [ 0.6488, -0.2874, -0.0944,  0.2792, -0.0366],\n",
      "        [ 0.6663, -0.3188, -0.0768,  0.3009, -0.0787]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "target: tensor([[0., 1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0.],\n",
      "        [0., 0., 0., 1., 0.],\n",
      "        [1., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0.]], device='cuda:0')\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output: tensor([[ 0.6973, -0.4057,  0.0027,  0.3547, -0.2938],\n",
      "        [ 0.9002, -0.5296,  0.0843,  0.2740, -0.3935],\n",
      "        [ 0.9322, -0.4342,  0.1171,  0.2458, -0.5910],\n",
      "        [ 0.8779, -0.3307,  0.0430,  0.2211, -0.5863],\n",
      "        [ 0.6537, -0.2673, -0.0121,  0.3180, -0.3325],\n",
      "        [ 0.8253, -0.3896,  0.1142,  0.2302, -0.5281],\n",
      "        [ 0.7816, -0.3399,  0.1225,  0.2413, -0.5844],\n",
      "        [ 0.9508, -0.5408,  0.1213,  0.2794, -0.4840]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "target: tensor([[0., 0., 0., 1., 0.],\n",
      "        [1., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0.],\n",
      "        [1., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1.],\n",
      "        [1., 0., 0., 0., 0.]], device='cuda:0')\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 9/54 [02:50<14:14, 19.00s/it]\n",
      "  0%|          | 0/10 [02:50<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\TaiSh\\Source\\CS_640-Artificial_Intelligence\\Final_Project\\Final_Project(2).ipynb Cell 10\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/TaiSh/Source/CS_640-Artificial_Intelligence/Final_Project/Final_Project%282%29.ipynb#X13sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m model_1\u001b[39m.\u001b[39mtrain()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/TaiSh/Source/CS_640-Artificial_Intelligence/Final_Project/Final_Project%282%29.ipynb#X13sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m tqdm(\u001b[39mrange\u001b[39m(num_epochs)):  \u001b[39m# Number of epochs\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/TaiSh/Source/CS_640-Artificial_Intelligence/Final_Project/Final_Project%282%29.ipynb#X13sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     \u001b[39mfor\u001b[39;49;00m data, target \u001b[39min\u001b[39;49;00m tqdm(train_dataloader):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/TaiSh/Source/CS_640-Artificial_Intelligence/Final_Project/Final_Project%282%29.ipynb#X13sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m         data \u001b[39m=\u001b[39;49m data\u001b[39m.\u001b[39;49mto(device)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/TaiSh/Source/CS_640-Artificial_Intelligence/Final_Project/Final_Project%282%29.ipynb#X13sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m         target \u001b[39m=\u001b[39;49m target\u001b[39m.\u001b[39;49mto(device)\n",
      "File \u001b[1;32mc:\\Users\\TaiSh\\.conda\\envs\\conda311\\Lib\\site-packages\\tqdm\\std.py:1178\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1175\u001b[0m time \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_time\n\u001b[0;32m   1177\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1178\u001b[0m     \u001b[39mfor\u001b[39;49;00m obj \u001b[39min\u001b[39;49;00m iterable:\n\u001b[0;32m   1179\u001b[0m         \u001b[39myield\u001b[39;49;00m obj\n\u001b[0;32m   1180\u001b[0m         \u001b[39m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[0;32m   1181\u001b[0m         \u001b[39m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
      "File \u001b[1;32mc:\\Users\\TaiSh\\.conda\\envs\\conda311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    627\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    628\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    629\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 630\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[0;32m    631\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    632\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    633\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\TaiSh\\.conda\\envs\\conda311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:674\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    672\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    673\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 674\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    675\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[0;32m    676\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\TaiSh\\.conda\\envs\\conda311\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;49;00m idx \u001b[39min\u001b[39;49;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\TaiSh\\.conda\\envs\\conda311\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "\u001b[1;32mc:\\Users\\TaiSh\\Source\\CS_640-Artificial_Intelligence\\Final_Project\\Final_Project(2).ipynb Cell 10\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/TaiSh/Source/CS_640-Artificial_Intelligence/Final_Project/Final_Project%282%29.ipynb#X13sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m one_hot \u001b[39m=\u001b[39m label_dict[label]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/TaiSh/Source/CS_640-Artificial_Intelligence/Final_Project/Final_Project%282%29.ipynb#X13sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransform:\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/TaiSh/Source/CS_640-Artificial_Intelligence/Final_Project/Final_Project%282%29.ipynb#X13sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m     image \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransform(image)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/TaiSh/Source/CS_640-Artificial_Intelligence/Final_Project/Final_Project%282%29.ipynb#X13sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m \u001b[39mreturn\u001b[39;00m image, one_hot\n",
      "File \u001b[1;32mc:\\Users\\TaiSh\\.conda\\envs\\conda311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\TaiSh\\.conda\\envs\\conda311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\TaiSh\\.conda\\envs\\conda311\\Lib\\site-packages\\torchvision\\transforms\\v2\\_container.py:53\u001b[0m, in \u001b[0;36mCompose.forward\u001b[1;34m(self, *inputs)\u001b[0m\n\u001b[0;32m     51\u001b[0m needs_unpacking \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(inputs) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m     52\u001b[0m \u001b[39mfor\u001b[39;00m transform \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransforms:\n\u001b[1;32m---> 53\u001b[0m     outputs \u001b[39m=\u001b[39m transform(\u001b[39m*\u001b[39;49minputs)\n\u001b[0;32m     54\u001b[0m     inputs \u001b[39m=\u001b[39m outputs \u001b[39mif\u001b[39;00m needs_unpacking \u001b[39melse\u001b[39;00m (outputs,)\n\u001b[0;32m     55\u001b[0m \u001b[39mreturn\u001b[39;00m outputs\n",
      "File \u001b[1;32mc:\\Users\\TaiSh\\.conda\\envs\\conda311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\TaiSh\\.conda\\envs\\conda311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\TaiSh\\.conda\\envs\\conda311\\Lib\\site-packages\\torchvision\\transforms\\v2\\_transform.py:50\u001b[0m, in \u001b[0;36mTransform.forward\u001b[1;34m(self, *inputs)\u001b[0m\n\u001b[0;32m     45\u001b[0m needs_transform_list \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_needs_transform_list(flat_inputs)\n\u001b[0;32m     46\u001b[0m params \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_params(\n\u001b[0;32m     47\u001b[0m     [inpt \u001b[39mfor\u001b[39;00m (inpt, needs_transform) \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(flat_inputs, needs_transform_list) \u001b[39mif\u001b[39;00m needs_transform]\n\u001b[0;32m     48\u001b[0m )\n\u001b[1;32m---> 50\u001b[0m flat_outputs \u001b[39m=\u001b[39m [\n\u001b[0;32m     51\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_transform(inpt, params) \u001b[39mif\u001b[39;49;00m needs_transform \u001b[39melse\u001b[39;49;00m inpt\n\u001b[0;32m     52\u001b[0m     \u001b[39mfor\u001b[39;49;00m (inpt, needs_transform) \u001b[39min\u001b[39;49;00m \u001b[39mzip\u001b[39;49m(flat_inputs, needs_transform_list)\n\u001b[0;32m     53\u001b[0m ]\n\u001b[0;32m     55\u001b[0m \u001b[39mreturn\u001b[39;00m tree_unflatten(flat_outputs, spec)\n",
      "File \u001b[1;32mc:\\Users\\TaiSh\\.conda\\envs\\conda311\\Lib\\site-packages\\torchvision\\transforms\\v2\\_transform.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     45\u001b[0m needs_transform_list \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_needs_transform_list(flat_inputs)\n\u001b[0;32m     46\u001b[0m params \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_params(\n\u001b[0;32m     47\u001b[0m     [inpt \u001b[39mfor\u001b[39;00m (inpt, needs_transform) \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(flat_inputs, needs_transform_list) \u001b[39mif\u001b[39;00m needs_transform]\n\u001b[0;32m     48\u001b[0m )\n\u001b[0;32m     50\u001b[0m flat_outputs \u001b[39m=\u001b[39m [\n\u001b[1;32m---> 51\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_transform(inpt, params) \u001b[39mif\u001b[39;00m needs_transform \u001b[39melse\u001b[39;00m inpt\n\u001b[0;32m     52\u001b[0m     \u001b[39mfor\u001b[39;00m (inpt, needs_transform) \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(flat_inputs, needs_transform_list)\n\u001b[0;32m     53\u001b[0m ]\n\u001b[0;32m     55\u001b[0m \u001b[39mreturn\u001b[39;00m tree_unflatten(flat_outputs, spec)\n",
      "File \u001b[1;32mc:\\Users\\TaiSh\\.conda\\envs\\conda311\\Lib\\site-packages\\torchvision\\transforms\\v2\\_geometry.py:159\u001b[0m, in \u001b[0;36mResize._transform\u001b[1;34m(self, inpt, params)\u001b[0m\n\u001b[0;32m    158\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_transform\u001b[39m(\u001b[39mself\u001b[39m, inpt: Any, params: Dict[\u001b[39mstr\u001b[39m, Any]) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[1;32m--> 159\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_kernel(\n\u001b[0;32m    160\u001b[0m         F\u001b[39m.\u001b[39;49mresize,\n\u001b[0;32m    161\u001b[0m         inpt,\n\u001b[0;32m    162\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msize,\n\u001b[0;32m    163\u001b[0m         interpolation\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minterpolation,\n\u001b[0;32m    164\u001b[0m         max_size\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_size,\n\u001b[0;32m    165\u001b[0m         antialias\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mantialias,\n\u001b[0;32m    166\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\TaiSh\\.conda\\envs\\conda311\\Lib\\site-packages\\torchvision\\transforms\\v2\\_transform.py:35\u001b[0m, in \u001b[0;36mTransform._call_kernel\u001b[1;34m(self, functional, inpt, *args, **kwargs)\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_call_kernel\u001b[39m(\u001b[39mself\u001b[39m, functional: Callable, inpt: Any, \u001b[39m*\u001b[39margs: Any, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[0;32m     34\u001b[0m     kernel \u001b[39m=\u001b[39m _get_kernel(functional, \u001b[39mtype\u001b[39m(inpt), allow_passthrough\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m---> 35\u001b[0m     \u001b[39mreturn\u001b[39;00m kernel(inpt, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\TaiSh\\.conda\\envs\\conda311\\Lib\\site-packages\\torchvision\\transforms\\v2\\functional\\_geometry.py:304\u001b[0m, in \u001b[0;36m__resize_image_pil_dispatch\u001b[1;34m(image, size, interpolation, max_size, antialias)\u001b[0m\n\u001b[0;32m    302\u001b[0m \u001b[39mif\u001b[39;00m antialias \u001b[39mis\u001b[39;00m \u001b[39mFalse\u001b[39;00m:\n\u001b[0;32m    303\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\u001b[39m\"\u001b[39m\u001b[39mAnti-alias option is always applied for PIL Image input. Argument antialias is ignored.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 304\u001b[0m \u001b[39mreturn\u001b[39;00m _resize_image_pil(image, size\u001b[39m=\u001b[39;49msize, interpolation\u001b[39m=\u001b[39;49minterpolation, max_size\u001b[39m=\u001b[39;49mmax_size)\n",
      "File \u001b[1;32mc:\\Users\\TaiSh\\.conda\\envs\\conda311\\Lib\\site-packages\\torchvision\\transforms\\v2\\functional\\_geometry.py:291\u001b[0m, in \u001b[0;36m_resize_image_pil\u001b[1;34m(image, size, interpolation, max_size)\u001b[0m\n\u001b[0;32m    288\u001b[0m \u001b[39mif\u001b[39;00m (new_height, new_width) \u001b[39m==\u001b[39m (old_height, old_width):\n\u001b[0;32m    289\u001b[0m     \u001b[39mreturn\u001b[39;00m image\n\u001b[1;32m--> 291\u001b[0m \u001b[39mreturn\u001b[39;00m image\u001b[39m.\u001b[39;49mresize((new_width, new_height), resample\u001b[39m=\u001b[39;49mpil_modes_mapping[interpolation])\n",
      "File \u001b[1;32mc:\\Users\\TaiSh\\.conda\\envs\\conda311\\Lib\\site-packages\\PIL\\Image.py:2138\u001b[0m, in \u001b[0;36mImage.resize\u001b[1;34m(self, size, resample, box, reducing_gap)\u001b[0m\n\u001b[0;32m   2134\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(msg)\n\u001b[0;32m   2136\u001b[0m size \u001b[39m=\u001b[39m \u001b[39mtuple\u001b[39m(size)\n\u001b[1;32m-> 2138\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mload()\n\u001b[0;32m   2139\u001b[0m \u001b[39mif\u001b[39;00m box \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   2140\u001b[0m     box \u001b[39m=\u001b[39m (\u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m) \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msize\n",
      "File \u001b[1;32mc:\\Users\\TaiSh\\.conda\\envs\\conda311\\Lib\\site-packages\\PIL\\ImageFile.py:269\u001b[0m, in \u001b[0;36mImageFile.load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    266\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mOSError\u001b[39;00m(msg)\n\u001b[0;32m    268\u001b[0m b \u001b[39m=\u001b[39m b \u001b[39m+\u001b[39m s\n\u001b[1;32m--> 269\u001b[0m n, err_code \u001b[39m=\u001b[39m decoder\u001b[39m.\u001b[39;49mdecode(b)\n\u001b[0;32m    270\u001b[0m \u001b[39mif\u001b[39;00m n \u001b[39m<\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m    271\u001b[0m     \u001b[39mbreak\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "\n",
    "losses = []\n",
    "\n",
    "\n",
    "# Train the Model\n",
    "#try:\n",
    "#    model_1 = torch.load('Cancer_CNN_1.pt')\n",
    "#    img = mpimg.imread('loss_1.png')\n",
    "#    imgplot = plt.imshow(img)\n",
    "#    plt.show()\n",
    "\n",
    "#except:\n",
    "model_1.train()\n",
    "for epoch in tqdm(range(num_epochs)):  # Number of epochs\n",
    "    for data, target in tqdm(train_dataloader):\n",
    "        data = data.to(device)\n",
    "        target = target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model_1(data)\n",
    "        print(f'output: {output}')\n",
    "        print(f'target: {target.type_as(output)}\\n')\n",
    "        loss = criterion(output, target.type_as(output)) # Ensuring target is same type as output\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    losses.append(loss.item())\n",
    "    print(f'Epoch {epoch+1}, Loss: {loss.item()}')\n",
    "torch.save(model_1, 'Cancer_CNN_1.pt')\n",
    "    \n",
    "fig = plt.figure()\n",
    "ax = plt.subplot(111)\n",
    "ax.plot(losses)\n",
    "plt.title('Loss')\n",
    "plt.show()\n",
    "fig.savefig('loss_1.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Initialize the CNN Model\n",
    "model_2 = CancerCNN_1().float()\n",
    "print(model_2)\n",
    "\n",
    "model_2 = model_2.to(device)\n",
    "\n",
    "# Step 5: Define Loss Function and Optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model_2.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 25\n",
    "\n",
    "losses = []\n",
    "\n",
    "\n",
    "# Train the Model\n",
    "#try:\n",
    "#    model_1 = torch.load('Cancer_CNN_1.pt')\n",
    "#    img = mpimg.imread('loss_1.png')\n",
    "#    imgplot = plt.imshow(img)\n",
    "#    plt.show()\n",
    "\n",
    "#except:\n",
    "model_2.train()\n",
    "for epoch in tqdm(range(num_epochs)):  # Number of epochs\n",
    "    for data, target in tqdm(train_dataloader):\n",
    "        data = data.to(device)\n",
    "        target = target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model_2(data)\n",
    "        print(f'output: {output}')\n",
    "        print(f'target: {target.type_as(output)}\\n')\n",
    "        loss = criterion(output, target.type_as(output)) # Ensuring target is same type as output\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    losses.append(loss.item())\n",
    "    print(f'Epoch {epoch+1}, Loss: {loss.item()}')\n",
    "torch.save(model_2, 'Cancer_CNN_2.pt')\n",
    "    \n",
    "fig = plt.figure()\n",
    "ax = plt.subplot(111)\n",
    "ax.plot(losses)\n",
    "plt.title('Loss')\n",
    "plt.show()\n",
    "fig.savefig('loss_2.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
